{"cells":[{"cell_type":"markdown","metadata":{"id":"QIuzds5LLPyC"},"source":["<a href=\"https://colab.research.google.com/github/mkshing/notebooks/blob/main/stable_video_diffusion_img2vid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","# Stable Video Diffusion (image-to-video) Demo\n","This notebook is the demo for the new image-to-video model, Stable Video Diffusion, from [Stability AI](https://stability.ai/) **on Colab free plan**.\n","\n","This was made by [mkshing](https://twitter.com/mk1stats).\n","\n","Visit the following links for the details of Stable Video Diffusion.\n","* Codebase: https://github.com/Stability-AI/generative-models\n","* HF:\n"," * SVD 1.0 (14 frames): https://huggingface.co/stabilityai/stable-video-diffusion-img2vid\n"," * SVD 1.0 (25 frames): https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt\n"," * SVD 1.1 (25 frames): https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1\n","* LICENSE: [STABLE VIDEO DIFFUSION NON-COMMERCIAL COMMUNITY LICENSE AGREEMENT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid/blob/main/LICENSE)\n","* Paper: https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets\n","\n","## Updates\n","### 2024.2.4\n","* Support [Stable Video Diffusion 1.1 Image-to-Video](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1)\n","\n","### 2023.11.27\n","* Add the other hyper-parameters (`fps_id`, `motion_bucket_id`, `cond_aug`)\n","\n","![000000](https://user-images.githubusercontent.com/33302880/284800538-f856b437-aa1f-4675-ba40-03da3e953358.gif)\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","id":"aaimSFWfLPgb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723470660843,"user_tz":-180,"elapsed":34518,"user":{"displayName":"ENES TURGUT","userId":"07545895200272138795"}},"outputId":"4e5cd62e-7db5-41f4-c835-d5cd189a2252"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Aug 12 13:50:27 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n","fatal: destination path 'generative-models' already exists and is not an empty directory.\n","--2024-08-12 13:50:27--  https://gist.githubusercontent.com/mkshing/4ad40699756d996ba6b3f7934e6ca532/raw/3f0094272c7a2bd3eb5f1a0db91bed582c9e8f01/requirements.txt\n","Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n","Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 746 [text/plain]\n","Saving to: ‘requirements.txt.1’\n","\n","requirements.txt.1  100%[===================>]     746  --.-KB/s    in 0s      \n","\n","2024-08-12 13:50:27 (77.9 MB/s) - ‘requirements.txt.1’ saved [746/746]\n","\n","Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n","Collecting clip@ git+https://github.com/openai/CLIP.git (from -r requirements.txt (line 4))\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-install-hax707uo/clip_18bc3dac9ce34d8da86f4e4bac1ce38b\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-install-hax707uo/clip_18bc3dac9ce34d8da86f4e4bac1ce38b\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: black==23.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (23.7.0)\n","Requirement already satisfied: chardet==5.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (5.1.0)\n","Requirement already satisfied: einops>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.8.0)\n","Requirement already satisfied: fairscale>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.4.13)\n","Requirement already satisfied: fire>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.6.0)\n","Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2024.6.1)\n","Requirement already satisfied: invisible-watermark>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.2.0)\n","Requirement already satisfied: kornia==0.6.9 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.6.9)\n","Requirement already satisfied: matplotlib>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (3.9.1.post1)\n","Requirement already satisfied: natsort>=8.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (8.4.0)\n","Requirement already satisfied: ninja>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.11.1.1)\n","Requirement already satisfied: omegaconf>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.3.0)\n","Requirement already satisfied: open-clip-torch>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (2.26.1)\n","Requirement already satisfied: opencv-python==4.6.0.66 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (4.6.0.66)\n","Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (2.1.4)\n","Requirement already satisfied: pillow>=9.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (10.4.0)\n","Requirement already satisfied: pudb>=2022.1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (2024.1.2)\n","Requirement already satisfied: pytorch-lightning==2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (2.0.1)\n","Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (6.0.2)\n","Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (1.13.1)\n","Requirement already satisfied: streamlit>=0.73.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (1.37.1)\n","Requirement already satisfied: tensorboardx==2.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (2.6)\n","Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (1.0.8)\n","Requirement already satisfied: tokenizers==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (0.12.1)\n","Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (2.0.1+cu118)\n","Requirement already satisfied: torchaudio>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 29)) (2.0.2+cu118)\n","Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 30)) (0.6.1)\n","Requirement already satisfied: torchmetrics>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 31)) (1.4.1)\n","Requirement already satisfied: torchvision>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 32)) (0.15.2+cu118)\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 33)) (4.66.5)\n","Requirement already satisfied: transformers==4.19.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 34)) (4.19.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (2.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 36)) (1.26.19)\n","Requirement already satisfied: wandb>=0.15.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 37)) (0.17.6)\n","Requirement already satisfied: webdataset>=0.2.33 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 38)) (0.2.86)\n","Requirement already satisfied: wheel>=0.41.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 39)) (0.44.0)\n","Requirement already satisfied: xformers>=0.0.20 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 40)) (0.0.22)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black==23.7.0->-r requirements.txt (line 2)) (8.1.7)\n","Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black==23.7.0->-r requirements.txt (line 2)) (1.0.0)\n","Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black==23.7.0->-r requirements.txt (line 2)) (24.1)\n","Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black==23.7.0->-r requirements.txt (line 2)) (0.12.1)\n","Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black==23.7.0->-r requirements.txt (line 2)) (4.2.2)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black==23.7.0->-r requirements.txt (line 2)) (2.0.1)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python==4.6.0.66->-r requirements.txt (line 17)) (1.26.4)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==2.0.1->-r requirements.txt (line 21)) (4.12.2)\n","Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==2.0.1->-r requirements.txt (line 21)) (0.11.6)\n","Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardx==2.6->-r requirements.txt (line 25)) (3.20.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->-r requirements.txt (line 30)) (2.32.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 28)) (3.15.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 28)) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 28)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 28)) (3.1.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.1->-r requirements.txt (line 34)) (0.23.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.1->-r requirements.txt (line 34)) (2024.5.15)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->-r requirements.txt (line 35)) (3.30.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->-r requirements.txt (line 35)) (18.1.8)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip@ git+https://github.com/openai/CLIP.git->-r requirements.txt (line 4)) (6.2.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire>=0.5.0->-r requirements.txt (line 7)) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.5.0->-r requirements.txt (line 7)) (2.4.0)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from invisible-watermark>=0.2.0->-r requirements.txt (line 9)) (1.6.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->-r requirements.txt (line 11)) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->-r requirements.txt (line 11)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->-r requirements.txt (line 11)) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->-r requirements.txt (line 11)) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->-r requirements.txt (line 11)) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.2->-r requirements.txt (line 11)) (2.8.2)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.3.0->-r requirements.txt (line 15)) (4.9.3)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->-r requirements.txt (line 18)) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->-r requirements.txt (line 18)) (2024.1)\n","Requirement already satisfied: urwid>=2.4 in /usr/local/lib/python3.10/dist-packages (from pudb>=2022.1.3->-r requirements.txt (line 20)) (2.6.15)\n","Requirement already satisfied: pygments>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from pudb>=2022.1.3->-r requirements.txt (line 20)) (2.16.1)\n","Requirement already satisfied: jedi<1,>=0.18 in /usr/local/lib/python3.10/dist-packages (from pudb>=2022.1.3->-r requirements.txt (line 20)) (0.19.1)\n","Requirement already satisfied: urwid-readline in /usr/local/lib/python3.10/dist-packages (from pudb>=2022.1.3->-r requirements.txt (line 20)) (0.14)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 24)) (4.2.2)\n","Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 24)) (1.4)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 24)) (5.4.0)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 24)) (14.0.2)\n","Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 24)) (13.7.1)\n","Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 24)) (8.5.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 24)) (0.10.2)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 24)) (3.1.43)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 24)) (0.9.1)\n","Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 24)) (6.3.3)\n","Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 24)) (4.0.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.2->-r requirements.txt (line 26)) (0.4.4)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.6->-r requirements.txt (line 37)) (0.4.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.6->-r requirements.txt (line 37)) (5.9.5)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.6->-r requirements.txt (line 37)) (2.12.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.6->-r requirements.txt (line 37)) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.15.6->-r requirements.txt (line 37)) (71.0.4)\n","Requirement already satisfied: braceexpand in /usr/local/lib/python3.10/dist-packages (from webdataset>=0.2.33->-r requirements.txt (line 38)) (0.1.7)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.73.1->-r requirements.txt (line 24)) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.73.1->-r requirements.txt (line 24)) (4.23.0)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.73.1->-r requirements.txt (line 24)) (0.12.1)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==2.0.1->-r requirements.txt (line 21)) (3.10.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.73.1->-r requirements.txt (line 24)) (4.0.11)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi<1,>=0.18->pudb>=2022.1.3->-r requirements.txt (line 20)) (0.8.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.1->-r requirements.txt (line 28)) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.1->-r requirements.txt (line 30)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.1->-r requirements.txt (line 30)) (3.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.1->-r requirements.txt (line 30)) (2024.7.4)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=0.73.1->-r requirements.txt (line 24)) (3.0.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from urwid>=2.4->pudb>=2022.1.3->-r requirements.txt (line 20)) (0.2.13)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.1->-r requirements.txt (line 28)) (1.3.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.1->-r requirements.txt (line 21)) (2.3.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.1->-r requirements.txt (line 21)) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.1->-r requirements.txt (line 21)) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.1->-r requirements.txt (line 21)) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.1->-r requirements.txt (line 21)) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.1->-r requirements.txt (line 21)) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==2.0.1->-r requirements.txt (line 21)) (4.0.3)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.73.1->-r requirements.txt (line 24)) (5.0.1)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.73.1->-r requirements.txt (line 24)) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.73.1->-r requirements.txt (line 24)) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.73.1->-r requirements.txt (line 24)) (0.20.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=0.73.1->-r requirements.txt (line 24)) (0.1.2)\n","Obtaining file:///content/generative-models\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: sgm\n","  Building editable for sgm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgm: filename=sgm-0.1.0-py3-none-any.whl size=30739 sha256=83bf99789aba1315d473fbd8ee272ccbe65997bb48d923c3a9480b50b624cb9a\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-gw3wdi5_/wheels/12/9b/27/03142f4dee9fa0a99f6c146eae81eb66e17b781145ecb05fa5\n","Successfully built sgm\n","Installing collected packages: sgm\n","Successfully installed sgm-0.1.0\n","Obtaining sdata from git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n","  Updating ./src/sdata clone (to revision main)\n","  Running command git fetch -q --tags\n","  Running command git reset --hard -q 8bce77d147033b3a5285b6d45ee85f33866964fc\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Installing collected packages: sdata\n","  Attempting uninstall: sdata\n","    Found existing installation: sdata 0.0.1\n","    Uninstalling sdata-0.0.1:\n","      Successfully uninstalled sdata-0.0.1\n","  Running setup.py develop for sdata\n","Successfully installed sdata-0.0.1\n","Collecting gradio\n","  Downloading gradio-4.41.0-py3-none-any.whl.metadata (15 kB)\n","Collecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Collecting fastapi (from gradio)\n","  Downloading fastapi-0.112.0-py3-none-any.whl.metadata (27 kB)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting gradio-client==1.3.0 (from gradio)\n","  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting httpx>=0.24.1 (from gradio)\n","  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.5)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.1.post1)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n","Collecting orjson~=3.0 (from gradio)\n","  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart>=0.0.9 (from gradio)\n","  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n","Collecting ruff>=0.2.2 (from gradio)\n","  Downloading ruff-0.5.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n","Collecting urllib3~=2.0 (from gradio)\n","  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.30.5-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n","Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n","Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n","Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n","  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Downloading gradio-4.41.0-py3-none-any.whl (12.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Downloading ruff-0.5.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.30.5-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fastapi-0.112.0-py3-none-any.whl (93 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n","Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pydub, websockets, urllib3, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.26.19\n","    Uninstalling urllib3-1.26.19:\n","      Successfully uninstalled urllib3-1.26.19\n","  Attempting uninstall: tomlkit\n","    Found existing installation: tomlkit 0.13.0\n","    Uninstalling tomlkit-0.13.0:\n","      Successfully uninstalled tomlkit-0.13.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiofiles-23.2.1 fastapi-0.112.0 ffmpy-0.4.0 gradio-4.41.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.9 ruff-0.5.7 semantic-version-2.10.0 starlette-0.37.2 tomlkit-0.12.0 urllib3-2.2.2 uvicorn-0.30.5 websockets-12.0\n"]}],"source":["#@title Setup\n","!nvidia-smi\n","!git clone https://github.com/Stability-AI/generative-models.git\n","# install required packages from pypi\n","# !pip3 install -r generative-models/requirements/pt2.txt\n","# manually install only necesarry packages for colab\n","!wget https://gist.githubusercontent.com/mkshing/4ad40699756d996ba6b3f7934e6ca532/raw/3f0094272c7a2bd3eb5f1a0db91bed582c9e8f01/requirements.txt\n","!pip3 install -r requirements.txt\n","!pip3 install -e generative-models\n","!pip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n","!pip3 install gradio"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"mawwnzWX246N","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["a1ab36005e0b4d9db38ac75a26c0d474","427caa919fa74c30934ab50d77291918","a757f45e2ac84cc9b273352be74e613d","5ee5549074834dcaa60f206e7d9c19e1","6ae230dc66cc4ced8551ef02aebdce2d","b3bba6d962774b638ef0b887313c3170","2be1a59cc1484647b240f1bc56dd6458","473ac49b1f3e43848dba5123c8ae0ba1","3fa205bbd46a4ba2a6cc22247156f7ca","3f0bd2578e524aad9251afc59b6c638c","f6701cfc63ab4f36abe72cea4f90c5a2","e3370523d17e4217a029140f234086e1","01a0f81363294cdaa90a0ac1bbf54698","8dc941adf795482a988c1e0ac9c146f2","e6b26c59e83043a7a1fe157e22db2308","249d4965661c40e48b3254e13941a6cd","c6bcbd94404e4cbabe33332c4f9e16e0","18ebd9143c34409a9c0801da66b75bd2","4f9d4ce24f1b4aa9a2286ee6aadb1f3c","c6a45e2f774244258e18f00992949aed","4eed81eaa9bb43e7a06dddba90d81eb8","183d4ee2a040413aa49a929f909f7f61","1d68086003584960af9f90b0da0fd739","6d0ab56f4acb491a998c905c284f3753","dae0c5694f1a40e791822a3ef3b486de","4ba943f9007b4b64b5c6d82567d84777","4780e1cfc6be4f40929fe256f4e52813","86e72d591c1742d5962e51d5bda60cbc","f8f837bdc32448d9a13bfbe7df548bb3"]},"executionInfo":{"status":"ok","timestamp":1723470661319,"user_tz":-180,"elapsed":479,"user":{"displayName":"ENES TURGUT","userId":"07545895200272138795"}},"outputId":"f40da001-eb2d-4a99-d506-607aa1c62d82"},"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1ab36005e0b4d9db38ac75a26c0d474"}},"metadata":{}}],"source":["#@title Login HuggingFace to download weights\n","#@markdown Please make sure to fill in the form in the model cards and accept it.\n","from huggingface_hub import login\n","login()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"zWlfaXvPbR1L","executionInfo":{"status":"ok","timestamp":1723470679602,"user_tz":-180,"elapsed":858,"user":{"displayName":"ENES TURGUT","userId":"07545895200272138795"}}},"outputs":[],"source":["#@title Colab hack for SVD\n","# !pip uninstall -y numpy\n","# !pip install -U numpy\n","!mkdir -p /content/scripts/util/detection\n","!ln -s /content/generative-models/scripts/util/detection/p_head_v1.npz /content/scripts/util/detection/p_head_v1.npz\n","!ln -s /content/generative-models/scripts/util/detection/w_head_v1.npz /content/scripts/util/detection/w_head_v1.npz"]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","id":"v8O2yR3BLHv6","colab":{"base_uri":"https://localhost:8080/","height":173,"referenced_widgets":["e99cf02a27ff477c8f78c03371ed475c","325170b3a8cc4affb396c17307a382aa","c2c5d501852f4beba1d6c8814f9d7655","3dacd45785ed42c69f38bcf58423326c","993af5fe366c44feb5c81904a346766c","5bca4d6f56d9473a923bc63a7b551b3c","4010d12088e64c3ab9144d38afe719ce","a78efcc906834acfb7d33bfa5da0939b","2b2ab5b6113d42779e3ed8cf13758d1a","0e8c5ded6234470498bc983096a4baa8","ceb38caea1ba473e8856eab2f80bfac2"]},"executionInfo":{"status":"ok","timestamp":1723470811968,"user_tz":-180,"elapsed":131218,"user":{"displayName":"ENES TURGUT","userId":"07545895200272138795"}},"outputId":"6914e44e-69c2-4b88-e85e-fa7570707315"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["svd.safetensors:   0%|          | 0.00/9.56G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e99cf02a27ff477c8f78c03371ed475c"}},"metadata":{}}],"source":["# @title Download weights\n","import os\n","import subprocess\n","from huggingface_hub import hf_hub_download\n","version = \"svd\" #@param [\"svd\", \"svd-xt\", \"svd-xt-1-1\"]\n","TYPE2PATH = {\n","    \"svd\": [\"stabilityai/stable-video-diffusion-img2vid\", \"svd.safetensors\"],\n","    \"svd-xt\": [\"stabilityai/stable-video-diffusion-img2vid-xt\", \"svd_xt.safetensors\"],\n","    \"svd-xt-1-1\": [\"stabilityai/stable-video-diffusion-img2vid-xt-1-1\", \"svd_xt_1_1.safetensors\"],\n","}\n","repo_id, fname = TYPE2PATH[version]\n","ckpt_dir = \"/content/checkpoints\"\n","ckpt_path = os.path.join(ckpt_dir, fname)\n","# @markdown This will take several minutes. <br>\n","# @markdown **Reference:**\n","# @markdown * `svd`: [stabilityai/stable-video-diffusion-img2vid](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid) for 14 frames generation\n","# @markdown * `svd-xt`: [stabilityai/stable-video-diffusion-img2vid-xt](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt) for 25 frames generation\n","# @markdown * `svd-xt-1-1`: [stabilityai/stable-video-diffusion-img2vid-xt-1-1](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1) for 25 frames generation with fixed conditioning at 6FPS and Motion Bucket Id 127\n","\n","os.makedirs(\"checkpoints\", exist_ok=True)\n","if os.path.exists(ckpt_path):\n","  print(\"Already downloaded\")\n","else:\n","  hf_hub_download(\n","      repo_id=repo_id,\n","      filename=fname,\n","      local_dir=ckpt_dir,\n","  )"]},{"cell_type":"code","execution_count":5,"metadata":{"cellView":"form","id":"9AZDrh-SUDt2","colab":{"base_uri":"https://localhost:8080/","height":465,"referenced_widgets":["cb1766c082a4414fb6ba6912c8f607fa","818fe165fe6c41c8913e7ea576ff7c4c","33a53046b9274f4e8b4e3bcc9e18bf14","52355f05cb7c4c8cb6b1ad0472325225","77cd85a5f7f34898adbff812e82a7a79","94c364b0b9294db8a614f7c6ddeff25c","f6ab2b01e0f14839a30d12a5ac23a83a","8db27224fcb74248989bef0d46678ed2","9a70eb0221964372938aef8b93e606d4","5bbbc5c5ff464492ae2762917b03e6ab","2d12a3d69c7b4923941f867925537df7"]},"executionInfo":{"status":"ok","timestamp":1723470924992,"user_tz":-180,"elapsed":113027,"user":{"displayName":"ENES TURGUT","userId":"07545895200272138795"}},"outputId":"c9858296-c1dc-4c12-cd69-351ab11f3c6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Changed `ckpt_path` to /content/checkpoints/svd.safetensors\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n","VideoTransformerBlock is using checkpointing\n"]},{"output_type":"display_data","data":{"text/plain":["open_clip_pytorch_model.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb1766c082a4414fb6ba6912c8f607fa"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Initialized embedder #0: FrozenOpenCLIPImagePredictionEmbedder with 683800065 params. Trainable: False\n","Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n","Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n","Initialized embedder #3: VideoPredictionEmbedderWithEncoder with 83653863 params. Trainable: False\n","Initialized embedder #4: ConcatTimestepEmbedderND with 0 params. Trainable: False\n","Restored from /content/checkpoints/svd.safetensors with 0 missing and 0 unexpected keys\n"]},{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 890M/890M [00:17<00:00, 54.0MiB/s]\n"]}],"source":["#@title Load Model\n","import sys\n","from omegaconf import OmegaConf\n","\n","import torch\n","\n","sys.path.append(\"generative-models\")\n","from sgm.util import default, instantiate_from_config\n","from scripts.util.detection.nsfw_and_watermark_dectection import DeepFloydDataFiltering\n","\n","def load_model(\n","    config: str,\n","    device: str,\n","    num_frames: int,\n","    num_steps: int,\n","    ckpt_path: str = None,\n","):\n","    config = OmegaConf.load(config)\n","    config.model.params.conditioner_config.params.emb_models[\n","        0\n","    ].params.open_clip_embedding_config.params.init_device = device\n","    config.model.params.sampler_config.params.num_steps = num_steps\n","    config.model.params.sampler_config.params.guider_config.params.num_frames = (\n","        num_frames\n","    )\n","    if ckpt_path is not None:\n","        config.model.params.ckpt_path = ckpt_path\n","        print(f\"Changed `ckpt_path` to {ckpt_path}\")\n","    with torch.device(device):\n","        model = instantiate_from_config(config.model).to(device).eval().requires_grad_(False)\n","\n","    filter = DeepFloydDataFiltering(verbose=False, device=device)\n","    return model, filter\n","\n","\n","if version == \"svd\":\n","    num_frames = 14\n","    num_steps = 25\n","    # output_folder = default(output_folder, \"outputs/simple_video_sample/svd/\")\n","    model_config = \"generative-models/scripts/sampling/configs/svd.yaml\"\n","elif \"svd-xt\" in version:\n","    num_frames = 25\n","    num_steps = 30\n","    # output_folder = default(output_folder, \"outputs/simple_video_sample/svd_xt/\")\n","    model_config = \"generative-models/scripts/sampling/configs/svd_xt.yaml\"\n","else:\n","    raise ValueError(f\"Version {version} does not exist.\")\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, filter = load_model(\n","    model_config,\n","    device,\n","    num_frames,\n","    num_steps,\n","    ckpt_path,\n",")\n","# move models expect unet to cpu\n","model.conditioner.cpu()\n","model.first_stage_model.cpu()\n","# change the dtype of unet\n","model.model.to(dtype=torch.float16)\n","torch.cuda.empty_cache()\n","model = model.requires_grad_(False)"]},{"cell_type":"code","execution_count":6,"metadata":{"cellView":"form","id":"x1-dnq0RT95O","executionInfo":{"status":"ok","timestamp":1723470925493,"user_tz":-180,"elapsed":505,"user":{"displayName":"ENES TURGUT","userId":"07545895200272138795"}}},"outputs":[],"source":["# @title Sampling function\n","import math\n","import os\n","from glob import glob\n","from pathlib import Path\n","from typing import Optional\n","\n","import cv2\n","import numpy as np\n","import torch\n","from einops import rearrange, repeat\n","from fire import Fire\n","\n","from PIL import Image\n","from torchvision.transforms import ToTensor\n","from torchvision.transforms import functional as TF\n","\n","from sgm.inference.helpers import embed_watermark\n","from sgm.util import default, instantiate_from_config\n","\n","\n","def get_unique_embedder_keys_from_conditioner(conditioner):\n","    return list(set([x.input_key for x in conditioner.embedders]))\n","\n","\n","def get_batch(keys, value_dict, N, T, device, dtype=None):\n","    batch = {}\n","    batch_uc = {}\n","\n","    for key in keys:\n","        if key == \"fps_id\":\n","            batch[key] = (\n","                torch.tensor([value_dict[\"fps_id\"]])\n","                .to(device, dtype=dtype)\n","                .repeat(int(math.prod(N)))\n","            )\n","        elif key == \"motion_bucket_id\":\n","            batch[key] = (\n","                torch.tensor([value_dict[\"motion_bucket_id\"]])\n","                .to(device, dtype=dtype)\n","                .repeat(int(math.prod(N)))\n","            )\n","        elif key == \"cond_aug\":\n","            batch[key] = repeat(\n","                torch.tensor([value_dict[\"cond_aug\"]]).to(device, dtype=dtype),\n","                \"1 -> b\",\n","                b=math.prod(N),\n","            )\n","        elif key == \"cond_frames\":\n","            batch[key] = repeat(value_dict[\"cond_frames\"], \"1 ... -> b ...\", b=N[0])\n","        elif key == \"cond_frames_without_noise\":\n","            batch[key] = repeat(\n","                value_dict[\"cond_frames_without_noise\"], \"1 ... -> b ...\", b=N[0]\n","            )\n","        else:\n","            batch[key] = value_dict[key]\n","\n","    if T is not None:\n","        batch[\"num_video_frames\"] = T\n","\n","    for key in batch.keys():\n","        if key not in batch_uc and isinstance(batch[key], torch.Tensor):\n","            batch_uc[key] = torch.clone(batch[key])\n","    return batch, batch_uc\n","\n","\n","\n","def sample(\n","    input_path: str = \"assets/test_image.png\",  # Can either be image file or folder with image files\n","    resize_image: bool = False,\n","    num_frames: Optional[int] = None,\n","    num_steps: Optional[int] = None,\n","    fps_id: int = 6,\n","    motion_bucket_id: int = 127,\n","    cond_aug: float = 0.02,\n","    seed: int = 23,\n","    decoding_t: int = 14,  # Number of frames decoded at a time! This eats most VRAM. Reduce if necessary.\n","    device: str = \"cuda\",\n","    output_folder: Optional[str] = \"/content/outputs\",\n","    skip_filter: bool = False,\n","):\n","    \"\"\"\n","    Simple script to generate a single sample conditioned on an image `input_path` or multiple images, one for each\n","    image file in folder `input_path`. If you run out of VRAM, try decreasing `decoding_t`.\n","    \"\"\"\n","    torch.manual_seed(seed)\n","\n","    path = Path(input_path)\n","    all_img_paths = []\n","    if path.is_file():\n","        if any([input_path.endswith(x) for x in [\"jpg\", \"jpeg\", \"png\"]]):\n","            all_img_paths = [input_path]\n","        else:\n","            raise ValueError(\"Path is not valid image file.\")\n","    elif path.is_dir():\n","        all_img_paths = sorted(\n","            [\n","                f\n","                for f in path.iterdir()\n","                if f.is_file() and f.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n","            ]\n","        )\n","        if len(all_img_paths) == 0:\n","            raise ValueError(\"Folder does not contain any images.\")\n","    else:\n","        raise ValueError\n","    all_out_paths = []\n","    for input_img_path in all_img_paths:\n","        with Image.open(input_img_path) as image:\n","            if image.mode == \"RGBA\":\n","                image = image.convert(\"RGB\")\n","            if resize_image and image.size != (1024, 576):\n","                print(f\"Resizing {image.size} to (1024, 576)\")\n","                image = TF.resize(TF.resize(image, 1024), (576, 1024))\n","            w, h = image.size\n","\n","            if h % 64 != 0 or w % 64 != 0:\n","                width, height = map(lambda x: x - x % 64, (w, h))\n","                image = image.resize((width, height))\n","                print(\n","                    f\"WARNING: Your image is of size {h}x{w} which is not divisible by 64. We are resizing to {height}x{width}!\"\n","                )\n","\n","            image = ToTensor()(image)\n","            image = image * 2.0 - 1.0\n","\n","        image = image.unsqueeze(0).to(device)\n","        H, W = image.shape[2:]\n","        assert image.shape[1] == 3\n","        F = 8\n","        C = 4\n","        shape = (num_frames, C, H // F, W // F)\n","        if (H, W) != (576, 1024):\n","            print(\n","                \"WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\"\n","            )\n","        if motion_bucket_id > 255:\n","            print(\n","                \"WARNING: High motion bucket! This may lead to suboptimal performance.\"\n","            )\n","\n","        if fps_id < 5:\n","            print(\"WARNING: Small fps value! This may lead to suboptimal performance.\")\n","\n","        if fps_id > 30:\n","            print(\"WARNING: Large fps value! This may lead to suboptimal performance.\")\n","\n","        value_dict = {}\n","        value_dict[\"motion_bucket_id\"] = motion_bucket_id\n","        value_dict[\"fps_id\"] = fps_id\n","        value_dict[\"cond_aug\"] = cond_aug\n","        value_dict[\"cond_frames_without_noise\"] = image\n","        value_dict[\"cond_frames\"] = image + cond_aug * torch.randn_like(image)\n","        value_dict[\"cond_aug\"] = cond_aug\n","        # low vram mode\n","        model.conditioner.cpu()\n","        model.first_stage_model.cpu()\n","        torch.cuda.empty_cache()\n","        model.sampler.verbose = True\n","\n","        with torch.no_grad():\n","            with torch.autocast(device):\n","                model.conditioner.to(device)\n","                batch, batch_uc = get_batch(\n","                    get_unique_embedder_keys_from_conditioner(model.conditioner),\n","                    value_dict,\n","                    [1, num_frames],\n","                    T=num_frames,\n","                    device=device,\n","                )\n","                c, uc = model.conditioner.get_unconditional_conditioning(\n","                    batch,\n","                    batch_uc=batch_uc,\n","                    force_uc_zero_embeddings=[\n","                        \"cond_frames\",\n","                        \"cond_frames_without_noise\",\n","                    ],\n","                )\n","                model.conditioner.cpu()\n","                torch.cuda.empty_cache()\n","\n","                # from here, dtype is fp16\n","                for k in [\"crossattn\", \"concat\"]:\n","                    uc[k] = repeat(uc[k], \"b ... -> b t ...\", t=num_frames)\n","                    uc[k] = rearrange(uc[k], \"b t ... -> (b t) ...\", t=num_frames)\n","                    c[k] = repeat(c[k], \"b ... -> b t ...\", t=num_frames)\n","                    c[k] = rearrange(c[k], \"b t ... -> (b t) ...\", t=num_frames)\n","                for k in uc.keys():\n","                    uc[k] = uc[k].to(dtype=torch.float16)\n","                    c[k] = c[k].to(dtype=torch.float16)\n","\n","                randn = torch.randn(shape, device=device, dtype=torch.float16)\n","\n","                additional_model_inputs = {}\n","                additional_model_inputs[\"image_only_indicator\"] = torch.zeros(\n","                    2, num_frames\n","                ).to(device, )\n","                additional_model_inputs[\"num_video_frames\"] = batch[\"num_video_frames\"]\n","\n","                for k in additional_model_inputs:\n","                    if isinstance(additional_model_inputs[k], torch.Tensor):\n","                        additional_model_inputs[k] = additional_model_inputs[k].to(dtype=torch.float16)\n","\n","                def denoiser(input, sigma, c):\n","                    return model.denoiser(\n","                        model.model, input, sigma, c, **additional_model_inputs\n","                    )\n","\n","                samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n","                samples_z.to(dtype=model.first_stage_model.dtype)\n","                ##\n","\n","                model.en_and_decode_n_samples_a_time = decoding_t\n","                model.first_stage_model.to(device)\n","                samples_x = model.decode_first_stage(samples_z)\n","                samples = torch.clamp((samples_x + 1.0) / 2.0, min=0.0, max=1.0)\n","                model.first_stage_model.cpu()\n","                torch.cuda.empty_cache()\n","\n","                os.makedirs(output_folder, exist_ok=True)\n","                base_count = len(glob(os.path.join(output_folder, \"*.mp4\")))\n","                video_path = os.path.join(output_folder, f\"{base_count:06d}.mp4\")\n","                writer = cv2.VideoWriter(\n","                    video_path,\n","                    cv2.VideoWriter_fourcc(*\"MP4V\"),\n","                    fps_id + 1,\n","                    (samples.shape[-1], samples.shape[-2]),\n","                )\n","\n","                samples = embed_watermark(samples)\n","                if not skip_filter:\n","                    samples = filter(samples)\n","                else:\n","                    print(\"WARNING: You have disabled the NSFW/Watermark filter. Please do not expose unfiltered results in services or applications open to the public.\")\n","                vid = (\n","                    (rearrange(samples, \"t c h w -> t h w c\") * 255)\n","                    .cpu()\n","                    .numpy()\n","                    .astype(np.uint8)\n","                )\n","                for frame in vid:\n","                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n","                    writer.write(frame)\n","                writer.release()\n","                all_out_paths.append(video_path)\n","    return all_out_paths\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MdVILPlMUDe","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3677ac4b-13c3-4398-916e-7cdb4f5e0c1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Running on public URL: https://ba6d35c3759efd1f0d.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://ba6d35c3759efd1f0d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["WARNING: Your image is of size 989x1011 which is not divisible by 64. We are resizing to 960x960!\n","WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\n","##############################  Sampling setting  ##############################\n","Sampler: EulerEDMSampler\n","Discretization: EDMDiscretization\n","Guider: LinearPredictionGuider\n"]},{"output_type":"stream","name":"stderr","text":["\rSampling with EulerEDMSampler for 26 steps:   0%|          | 0/26 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n","Sampling with EulerEDMSampler for 26 steps:   0%|          | 0/26 [00:01<?, ?it/s]\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 536, in process_events\n","    response = await route_utils.call_process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 288, in call_process_api\n","    output = await app.get_blocks().process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1931, in process_api\n","    result = await self.call_function(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1516, in call_function\n","    prediction = await anyio.to_thread.run_sync(  # type: ignore\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n","    result = context.run(func, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 826, in wrapper\n","    response = f(*args, **kwargs)\n","  File \"<ipython-input-7-fb3fbbbc57b1>\", line 17, in infer\n","    output_paths = sample(\n","  File \"<ipython-input-6-b54a611e3dac>\", line 209, in sample\n","    samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n","  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 120, in __call__\n","    x = self.sampler_step(\n","  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 99, in sampler_step\n","    denoised = self.denoise(x, denoiser, sigma_hat, cond, uc)\n","  File \"/content/generative-models/sgm/modules/diffusionmodules/sampling.py\", line 55, in denoise\n","    denoised = denoiser(*self.guider.prepare_inputs(x, sigma, cond, uc))\n","  File \"<ipython-input-6-b54a611e3dac>\", line 205, in denoiser\n","    return model.denoiser(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/diffusionmodules/denoiser.py\", line 37, in forward\n","    network(input * c_in, c_noise, cond, **additional_model_inputs) * c_out\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/diffusionmodules/wrappers.py\", line 39, in forward\n","    return self.diffusion_model(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/diffusionmodules/video_model.py\", line 470, in forward\n","    h = module(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/diffusionmodules/openaimodel.py\", line 116, in forward\n","    x = layer(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/video_attention.py\", line 282, in forward\n","    x = block(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/attention.py\", line 546, in forward\n","    return checkpoint(self._forward, x, context)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\n","    return CheckpointFunction.apply(function, preserve, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 506, in apply\n","    return super().apply(*args, **kwargs)  # type: ignore[misc]\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 107, in forward\n","    outputs = run_function(*args)\n","  File \"/content/generative-models/sgm/modules/attention.py\", line 571, in _forward\n","    x = self.ff(self.norm3(x)) + x\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/attention.py\", line 113, in forward\n","    return self.net(x)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n","    input = module(input)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/attention.py\", line 94, in forward\n","    return x * F.gelu(gate)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.72 GiB (GPU 0; 14.75 GiB total capacity; 11.66 GiB already allocated; 1.33 GiB free; 13.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"]},{"output_type":"stream","name":"stdout","text":["WARNING: Your image is of size 989x1011 which is not divisible by 64. We are resizing to 960x960!\n","WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 536, in process_events\n","    response = await route_utils.call_process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 288, in call_process_api\n","    output = await app.get_blocks().process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1931, in process_api\n","    result = await self.call_function(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1516, in call_function\n","    prediction = await anyio.to_thread.run_sync(  # type: ignore\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n","    result = context.run(func, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 826, in wrapper\n","    response = f(*args, **kwargs)\n","  File \"<ipython-input-7-fb3fbbbc57b1>\", line 17, in infer\n","    output_paths = sample(\n","  File \"<ipython-input-6-b54a611e3dac>\", line 171, in sample\n","    c, uc = model.conditioner.get_unconditional_conditioning(\n","  File \"/content/generative-models/sgm/modules/encoders/modules.py\", line 183, in get_unconditional_conditioning\n","    c = self(batch_c, force_cond_zero_embeddings)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/encoders/modules.py\", line 132, in forward\n","    emb_out = embedder(batch[embedder.input_key])\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/encoders/modules.py\", line 1019, in forward\n","    out = self.encoder.encode(vid[n * n_samples : (n + 1) * n_samples])\n","  File \"/content/generative-models/sgm/models/autoencoder.py\", line 472, in encode\n","    z = self.encoder(x)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/diffusionmodules/model.py\", line 584, in forward\n","    h = self.down[i_level].block[i_block](hs[-1], temb)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/diffusionmodules/model.py\", line 134, in forward\n","    h = nonlinearity(h)\n","  File \"/content/generative-models/sgm/modules/diffusionmodules/model.py\", line 49, in nonlinearity\n","    return x * torch.sigmoid(x)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 450.00 MiB (GPU 0; 14.75 GiB total capacity; 14.21 GiB already allocated; 275.06 MiB free; 14.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"]},{"output_type":"stream","name":"stdout","text":["WARNING: Your image is of size 977x788 which is not divisible by 64. We are resizing to 960x768!\n","WARNING: The conditioning frame you provided is not 576x1024. This leads to suboptimal performance as model was only trained on 576x1024. Consider increasing `cond_aug`.\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 536, in process_events\n","    response = await route_utils.call_process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 288, in call_process_api\n","    output = await app.get_blocks().process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1931, in process_api\n","    result = await self.call_function(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1516, in call_function\n","    prediction = await anyio.to_thread.run_sync(  # type: ignore\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n","    result = context.run(func, *args)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 826, in wrapper\n","    response = f(*args, **kwargs)\n","  File \"<ipython-input-7-fb3fbbbc57b1>\", line 17, in infer\n","    output_paths = sample(\n","  File \"<ipython-input-6-b54a611e3dac>\", line 171, in sample\n","    c, uc = model.conditioner.get_unconditional_conditioning(\n","  File \"/content/generative-models/sgm/modules/encoders/modules.py\", line 183, in get_unconditional_conditioning\n","    c = self(batch_c, force_cond_zero_embeddings)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/encoders/modules.py\", line 132, in forward\n","    emb_out = embedder(batch[embedder.input_key])\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/encoders/modules.py\", line 1019, in forward\n","    out = self.encoder.encode(vid[n * n_samples : (n + 1) * n_samples])\n","  File \"/content/generative-models/sgm/models/autoencoder.py\", line 472, in encode\n","    z = self.encoder(x)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/diffusionmodules/model.py\", line 584, in forward\n","    h = self.down[i_level].block[i_block](hs[-1], temb)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/generative-models/sgm/modules/diffusionmodules/model.py\", line 134, in forward\n","    h = nonlinearity(h)\n","  File \"/content/generative-models/sgm/modules/diffusionmodules/model.py\", line 49, in nonlinearity\n","    return x * torch.sigmoid(x)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 360.00 MiB (GPU 0; 14.75 GiB total capacity; 14.28 GiB already allocated; 177.06 MiB free; 14.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"]}],"source":["# @title Do the Run!\n","# @markdown Generation takes about 10 mins for 25 frames on T4 (Colab free plan). Please be patient...\n","# @markdown (V100 takes about 3 mins.)\n","import gradio as gr\n","import random\n","\n","\n","def infer(input_path: str, resize_image: bool, n_frames: int, n_steps: int, seed: str, decoding_t: int, fps_id: int, motion_bucket_id: int, cond_aug: float, skip_filter: bool = False) -> str:\n","  if seed == \"random\":\n","    seed = random.randint(0, 2**32)\n","  if version == \"svd-xt-1-1\":\n","    if fps_id != 6:\n","      print(\"[WARNING] svd-xt-1-1 was fine-tuned in fixed conditioning (`fps_id=6`, `motion_bucket_id=127`)! The performance may vary compared to SVD 1.0.\")\n","    if motion_bucket_id != 127:\n","      print(\"[WARNING] svd-xt-1-1 was fine-tuned in fixed conditioning (`fps_id=6`, `motion_bucket_id=127`)! The performance may vary compared to SVD 1.0.\")\n","  seed = int(seed)\n","  output_paths = sample(\n","    input_path=input_path,\n","    resize_image=resize_image,\n","    num_frames=n_frames,\n","    num_steps=n_steps,\n","    fps_id=fps_id,\n","    motion_bucket_id=motion_bucket_id,\n","    cond_aug=cond_aug,\n","    seed=seed,\n","    decoding_t=decoding_t,  # Number of frames decoded at a time! This eats most VRAM. Reduce if necessary.\n","    device=device,\n","    skip_filter=skip_filter,\n","  )\n","  return output_paths[0]\n","\n","\n","with gr.Blocks() as demo:\n","  with gr.Column():\n","    image = gr.Image(label=\"input image\", type=\"filepath\")\n","    resize_image = gr.Checkbox(label=\"resize to optimal size\", value=True)\n","    btn = gr.Button(\"Run\")\n","    with gr.Accordion(label=\"Advanced options\", open=False):\n","      n_frames = gr.Number(precision=0, label=\"number of frames\", value=num_frames)\n","      n_steps = gr.Number(precision=0, label=\"number of steps\", value=num_steps)\n","      seed = gr.Text(value=\"random\", label=\"seed (integer or 'random')\",)\n","      decoding_t = gr.Number(precision=0, label=\"number of frames decoded at a time\", value=2)\n","      fps_id = gr.Number(precision=0, label=\"frames per second\", value=6)\n","      motion_bucket_id = gr.Number(precision=0, value=127, label=\"motion bucket id\")\n","      cond_aug = gr.Number(label=\"condition augmentation factor\", value=0.02)\n","      skip_filter = gr.Checkbox(value=False, label=\"skip nsfw/watermark filter\")\n","  with gr.Column():\n","    video_out = gr.Video(label=\"generated video\")\n","  examples = [\n","      [\"https://user-images.githubusercontent.com/33302880/284758167-367a25d8-8d7b-42d3-8391-6d82813c7b0f.png\"],\n","  ]\n","  inputs = [image, resize_image, n_frames, n_steps, seed, decoding_t, fps_id, motion_bucket_id, cond_aug, skip_filter]\n","  outputs = [video_out]\n","  btn.click(infer, inputs=inputs, outputs=outputs)\n","  gr.Examples(examples=examples, inputs=inputs, outputs=outputs, fn=infer)\n","  demo.queue().launch(debug=True, share=True, show_error=True)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/mkshing/notebooks/blob/main/stable_video_diffusion_img2vid.ipynb","timestamp":1723453736906},{"file_id":"https://github.com/mkshing/notebooks/blob/main/stable_video_diffusion_img2vid.ipynb","timestamp":1707026454019}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a1ab36005e0b4d9db38ac75a26c0d474":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_4eed81eaa9bb43e7a06dddba90d81eb8","IPY_MODEL_183d4ee2a040413aa49a929f909f7f61","IPY_MODEL_1d68086003584960af9f90b0da0fd739"],"layout":"IPY_MODEL_2be1a59cc1484647b240f1bc56dd6458"}},"427caa919fa74c30934ab50d77291918":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_473ac49b1f3e43848dba5123c8ae0ba1","placeholder":"​","style":"IPY_MODEL_3fa205bbd46a4ba2a6cc22247156f7ca","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"a757f45e2ac84cc9b273352be74e613d":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_3f0bd2578e524aad9251afc59b6c638c","placeholder":"​","style":"IPY_MODEL_f6701cfc63ab4f36abe72cea4f90c5a2","value":""}},"5ee5549074834dcaa60f206e7d9c19e1":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_e3370523d17e4217a029140f234086e1","style":"IPY_MODEL_01a0f81363294cdaa90a0ac1bbf54698","value":false}},"6ae230dc66cc4ced8551ef02aebdce2d":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_8dc941adf795482a988c1e0ac9c146f2","style":"IPY_MODEL_e6b26c59e83043a7a1fe157e22db2308","tooltip":""}},"b3bba6d962774b638ef0b887313c3170":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_249d4965661c40e48b3254e13941a6cd","placeholder":"​","style":"IPY_MODEL_c6bcbd94404e4cbabe33332c4f9e16e0","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"2be1a59cc1484647b240f1bc56dd6458":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"473ac49b1f3e43848dba5123c8ae0ba1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fa205bbd46a4ba2a6cc22247156f7ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f0bd2578e524aad9251afc59b6c638c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6701cfc63ab4f36abe72cea4f90c5a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3370523d17e4217a029140f234086e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01a0f81363294cdaa90a0ac1bbf54698":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8dc941adf795482a988c1e0ac9c146f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6b26c59e83043a7a1fe157e22db2308":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"249d4965661c40e48b3254e13941a6cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6bcbd94404e4cbabe33332c4f9e16e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18ebd9143c34409a9c0801da66b75bd2":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f9d4ce24f1b4aa9a2286ee6aadb1f3c","placeholder":"​","style":"IPY_MODEL_c6a45e2f774244258e18f00992949aed","value":"Connecting..."}},"4f9d4ce24f1b4aa9a2286ee6aadb1f3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6a45e2f774244258e18f00992949aed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4eed81eaa9bb43e7a06dddba90d81eb8":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d0ab56f4acb491a998c905c284f3753","placeholder":"​","style":"IPY_MODEL_dae0c5694f1a40e791822a3ef3b486de","value":"Token is valid (permission: fineGrained)."}},"183d4ee2a040413aa49a929f909f7f61":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ba943f9007b4b64b5c6d82567d84777","placeholder":"​","style":"IPY_MODEL_4780e1cfc6be4f40929fe256f4e52813","value":"Your token has been saved to /root/.cache/huggingface/token"}},"1d68086003584960af9f90b0da0fd739":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86e72d591c1742d5962e51d5bda60cbc","placeholder":"​","style":"IPY_MODEL_f8f837bdc32448d9a13bfbe7df548bb3","value":"Login successful"}},"6d0ab56f4acb491a998c905c284f3753":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dae0c5694f1a40e791822a3ef3b486de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ba943f9007b4b64b5c6d82567d84777":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4780e1cfc6be4f40929fe256f4e52813":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86e72d591c1742d5962e51d5bda60cbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8f837bdc32448d9a13bfbe7df548bb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e99cf02a27ff477c8f78c03371ed475c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_325170b3a8cc4affb396c17307a382aa","IPY_MODEL_c2c5d501852f4beba1d6c8814f9d7655","IPY_MODEL_3dacd45785ed42c69f38bcf58423326c"],"layout":"IPY_MODEL_993af5fe366c44feb5c81904a346766c"}},"325170b3a8cc4affb396c17307a382aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bca4d6f56d9473a923bc63a7b551b3c","placeholder":"​","style":"IPY_MODEL_4010d12088e64c3ab9144d38afe719ce","value":"svd.safetensors: 100%"}},"c2c5d501852f4beba1d6c8814f9d7655":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a78efcc906834acfb7d33bfa5da0939b","max":9559625980,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b2ab5b6113d42779e3ed8cf13758d1a","value":9559625980}},"3dacd45785ed42c69f38bcf58423326c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e8c5ded6234470498bc983096a4baa8","placeholder":"​","style":"IPY_MODEL_ceb38caea1ba473e8856eab2f80bfac2","value":" 9.56G/9.56G [02:09&lt;00:00, 65.3MB/s]"}},"993af5fe366c44feb5c81904a346766c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bca4d6f56d9473a923bc63a7b551b3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4010d12088e64c3ab9144d38afe719ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a78efcc906834acfb7d33bfa5da0939b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b2ab5b6113d42779e3ed8cf13758d1a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e8c5ded6234470498bc983096a4baa8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ceb38caea1ba473e8856eab2f80bfac2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb1766c082a4414fb6ba6912c8f607fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_818fe165fe6c41c8913e7ea576ff7c4c","IPY_MODEL_33a53046b9274f4e8b4e3bcc9e18bf14","IPY_MODEL_52355f05cb7c4c8cb6b1ad0472325225"],"layout":"IPY_MODEL_77cd85a5f7f34898adbff812e82a7a79"}},"818fe165fe6c41c8913e7ea576ff7c4c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94c364b0b9294db8a614f7c6ddeff25c","placeholder":"​","style":"IPY_MODEL_f6ab2b01e0f14839a30d12a5ac23a83a","value":"open_clip_pytorch_model.bin: 100%"}},"33a53046b9274f4e8b4e3bcc9e18bf14":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8db27224fcb74248989bef0d46678ed2","max":3944692325,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a70eb0221964372938aef8b93e606d4","value":3944692325}},"52355f05cb7c4c8cb6b1ad0472325225":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bbbc5c5ff464492ae2762917b03e6ab","placeholder":"​","style":"IPY_MODEL_2d12a3d69c7b4923941f867925537df7","value":" 3.94G/3.94G [00:24&lt;00:00, 146MB/s]"}},"77cd85a5f7f34898adbff812e82a7a79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94c364b0b9294db8a614f7c6ddeff25c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6ab2b01e0f14839a30d12a5ac23a83a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8db27224fcb74248989bef0d46678ed2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a70eb0221964372938aef8b93e606d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5bbbc5c5ff464492ae2762917b03e6ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d12a3d69c7b4923941f867925537df7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}